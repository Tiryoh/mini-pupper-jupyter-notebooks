{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31441b12-0ff0-420e-974c-4ed5664b9c32",
   "metadata": {},
   "source": [
    "PyTorch Hubを利用します\n",
    "* https://pytorch.org/docs/stable/hub.html\n",
    "* https://pytorch.org/hub/\n",
    "\n",
    "今回使うのはPytorch Teamが公開するMobileNet v2のモデルです\n",
    "* https://pytorch.org/hub/pytorch_vision_mobilenet_v2/\n",
    "\n",
    "サンプルコードはBSD-3-Clauseライセンスで公開されている以下のサンプルを参照しています。  \n",
    "https://github.com/pytorch/pytorch.github.io/blob/site/assets/hub/pytorch_vision_mobilenet_v2.ipynb\n",
    "`Copyright (c) 2018, Facebook Inc`  \n",
    "https://github.com/pytorch/pytorch.github.io/blob/site/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de79156c-dd69-4239-b34d-83022ab96f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed903e1-2f4f-4d74-af95-6e2d22f729c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc65e5ab-4d21-4d34-8faf-d96eb55a9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec0fcf3-9b0e-441a-a486-07893ff628a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32078609b55a4ee8801c116c28c1b9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# モデルをダウンロードする\n",
    "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decb4649-fe0e-49ce-89a6-34579aebdceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを読み込む\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324860ba-94f1-461e-b59f-ece45110aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90231dac-2476-4dd5-b8e6-e2e042d4107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0977e+00, -1.7348e+00, -2.2355e+00, -2.9669e+00, -2.3805e+00,\n",
      "         9.7398e-01, -1.6049e+00,  3.6914e+00,  6.3812e+00, -1.2929e+00,\n",
      "        -6.7555e+00, -3.3525e+00, -7.9619e+00, -4.4554e+00, -5.6423e+00,\n",
      "        -4.6624e+00, -1.9577e+00, -3.5811e-01, -1.2812e+00, -4.6707e+00,\n",
      "        -3.2935e+00, -2.5674e+00, -2.4351e+00, -1.3017e+00, -3.2453e+00,\n",
      "        -1.4237e+00, -1.2001e+00,  4.1275e-01, -1.6093e+00,  1.5871e+00,\n",
      "         2.7725e-01, -6.2651e-01, -2.9734e-01, -3.8219e+00, -1.5450e+00,\n",
      "        -2.8976e+00, -5.6528e-01, -2.3938e+00, -3.3704e-01,  1.2809e+00,\n",
      "        -1.2516e+00, -2.6469e+00, -3.1011e+00, -2.2447e+00, -4.4385e-01,\n",
      "        -1.2620e+00,  8.2895e-01, -2.0436e+00, -6.6037e-01, -8.6518e-02,\n",
      "         4.8967e-01, -1.7190e+00, -7.7943e-01, -1.1046e+00, -5.3857e-01,\n",
      "        -2.9254e+00, -1.9327e+00, -2.7273e+00, -6.0903e-01, -1.6802e+00,\n",
      "         1.3443e+00, -4.2062e+00, -1.4768e+00, -4.5581e+00, -3.2726e+00,\n",
      "        -4.0086e+00,  1.5702e-01, -1.9921e+00, -7.4553e-01, -4.2230e+00,\n",
      "        -3.8855e+00, -9.4837e-01, -2.1373e+00, -3.5562e+00, -2.4602e+00,\n",
      "        -3.2339e+00, -3.1414e+00, -2.6786e+00, -3.4320e-01,  1.3021e+00,\n",
      "        -1.8081e+00, -6.9590e-01,  6.0993e-01,  1.0629e+00,  7.0009e-01,\n",
      "        -2.1399e+00, -8.3321e-01, -1.2712e+00, -2.9445e+00,  2.6159e+00,\n",
      "        -3.0859e+00, -3.9236e+00, -5.0966e+00, -2.6899e+00, -4.2541e+00,\n",
      "        -5.7496e+00, -1.4612e+00, -3.3379e+00, -5.1146e+00,  1.3198e-01,\n",
      "         6.4283e-01, -3.8178e+00, -6.0667e-01, -3.8236e+00,  8.3375e+00,\n",
      "        -2.0877e-01,  1.5727e+00, -3.2569e+00, -1.3088e+00, -4.3541e+00,\n",
      "        -1.8997e+00, -4.4558e+00, -2.6055e+00, -4.8200e-01,  6.5847e-01,\n",
      "        -1.1139e+00, -1.9922e+00, -2.9861e+00,  6.3543e-02, -4.1180e-02,\n",
      "        -4.9799e+00, -3.5623e-01,  6.9235e-02,  2.6186e-02,  1.0603e+00,\n",
      "        -3.7500e+00, -2.2764e+00, -1.1260e+00, -3.6557e+00, -1.0141e+00,\n",
      "        -3.8173e+00, -2.6216e+00,  8.1061e-01, -4.8440e+00, -1.8047e-01,\n",
      "        -3.7860e+00, -2.7942e+00, -4.0452e+00, -2.7590e+00, -7.6154e+00,\n",
      "        -6.3294e+00, -3.7503e+00, -4.7911e+00, -2.9558e+00, -1.1693e+00,\n",
      "        -1.1443e+00,  4.1418e-01, -2.1690e+00, -2.2886e+00, -1.6754e+00,\n",
      "        -2.3970e+00,  6.1470e+00,  6.3288e+00,  4.1129e+00,  5.7730e+00,\n",
      "         1.3963e+00,  3.3143e-01,  8.0281e+00,  1.5755e+00, -1.1561e-02,\n",
      "         3.4564e-02, -7.4600e-01,  5.0004e-01, -1.5472e+00, -2.2847e+00,\n",
      "        -3.6398e+00, -6.4526e-01, -2.2491e+00,  1.6223e-01,  4.6008e+00,\n",
      "         3.4056e+00,  7.1300e-02, -1.5552e+00,  5.0138e+00,  5.8507e+00,\n",
      "         1.5491e+00, -2.0445e+00,  2.4170e+00, -2.4798e+00,  1.2948e+00,\n",
      "         1.9327e+00, -2.7864e+00,  1.2956e+00,  2.8686e-01,  2.5281e+00,\n",
      "         4.3597e+00,  5.6367e+00, -6.1375e-01,  3.3545e+00, -2.7333e-01,\n",
      "         2.3450e+00, -1.3377e+00,  5.4958e+00,  4.5115e+00,  2.6891e-01,\n",
      "         1.1485e+00, -1.2564e-01,  1.1168e+00,  6.3615e-02,  6.5230e+00,\n",
      "         2.4672e+00,  4.8818e-01, -3.3207e-01,  9.2125e+00,  1.6410e+00,\n",
      "         1.3188e+00,  1.9943e-01,  4.5677e+00,  2.1875e+00,  2.2087e-01,\n",
      "        -2.4104e+00,  6.0055e-01,  3.0084e+00, -5.4444e-01, -1.2983e+00,\n",
      "         2.8078e+00,  1.9668e+00,  2.0604e+00,  7.7008e-01,  1.6875e+00,\n",
      "         1.9477e+00, -7.9099e-01,  7.2126e+00,  9.0092e+00,  8.0076e+00,\n",
      "         2.3563e+00,  3.7544e+00,  5.7382e+00,  3.8081e+00,  6.5486e+00,\n",
      "         9.7228e+00,  1.0013e+01,  9.5305e+00,  2.3904e+00, -5.0377e-01,\n",
      "         5.1103e+00,  5.7608e-01,  1.5447e+00,  1.2857e+00,  3.5652e+00,\n",
      "         2.5450e+00,  1.4888e+00, -8.1306e-01, -2.7417e+00,  8.7089e-01,\n",
      "        -5.8501e-01, -4.0767e-01,  3.2671e+00,  9.6768e+00,  8.9972e+00,\n",
      "         9.2585e+00,  1.4055e+00,  1.1182e-01,  2.6599e+00, -1.4223e+00,\n",
      "         1.1165e+00,  5.2566e+00,  9.9246e+00,  1.4355e+01,  1.1880e+01,\n",
      "         5.9999e+00,  1.0196e+01, -3.5385e-01,  8.2910e+00,  6.0059e+00,\n",
      "         6.8347e-01,  4.1931e-01,  2.0777e+00, -5.5983e-01,  4.8030e+00,\n",
      "         9.0072e+00,  1.7113e+00,  2.3010e+00,  4.5991e+00,  3.6477e+00,\n",
      "        -9.0076e-01,  8.6037e-01,  4.4647e+00,  2.8593e+00,  9.2391e+00,\n",
      "         3.4026e+00,  3.9940e+00,  3.7692e+00,  7.0309e+00,  3.9052e+00,\n",
      "         4.3277e+00, -3.0220e-01,  3.9064e+00,  1.5825e-01,  1.4127e+00,\n",
      "        -2.0500e+00, -1.2154e+00,  1.4670e+00, -1.4580e+00, -1.5738e+00,\n",
      "        -1.3263e+00,  9.2755e-02, -2.6180e+00, -1.4926e-01, -7.5717e-01,\n",
      "        -5.3535e+00, -3.6209e+00, -2.3243e+00, -1.6823e+00, -3.7550e+00,\n",
      "        -3.8941e+00, -7.9254e-01, -1.5161e+00, -5.7227e+00, -2.8694e+00,\n",
      "        -7.2898e-01, -4.1101e-01, -7.2024e-01,  3.7913e-02, -1.0243e+00,\n",
      "        -3.3016e+00, -3.5729e+00, -1.9918e+00, -1.6733e+00, -5.8971e+00,\n",
      "        -5.3597e+00, -3.9750e+00, -3.6543e+00, -4.1491e+00, -2.9045e+00,\n",
      "        -1.0445e+00, -2.6859e+00, -1.9066e+00, -2.3439e+00, -6.2355e-01,\n",
      "         3.5225e+00,  6.2726e+00,  8.5430e+00,  5.0669e+00,  1.8273e+00,\n",
      "         4.6736e+00,  8.4476e-01, -1.9574e-02,  3.3690e+00, -1.0283e-01,\n",
      "        -2.4612e+00,  2.4195e+00, -5.1352e-01, -3.5476e+00, -3.5470e+00,\n",
      "         2.0304e-01, -1.3766e+00, -2.6088e+00,  2.4946e+00, -2.3945e+00,\n",
      "        -1.1865e+00, -4.9946e+00, -3.3072e+00, -4.2007e-01, -3.7238e+00,\n",
      "         5.3880e+00,  3.4567e+00,  1.0361e+00,  3.7673e+00,  4.1483e+00,\n",
      "        -6.8158e-01,  2.9724e+00, -2.6524e-01,  8.0892e-01, -1.7007e+00,\n",
      "        -2.0444e+00, -2.9546e+00, -3.2660e+00, -8.8699e-01, -3.3023e+00,\n",
      "        -3.3806e-01,  1.2563e+00, -8.9535e-01,  1.4541e+00,  9.4996e-01,\n",
      "        -2.0507e+00, -4.4067e+00,  2.2796e+00, -6.8593e-01, -8.8651e-01,\n",
      "         1.5170e+00, -3.8388e-01, -4.0742e-01,  9.6686e-01,  1.2590e+00,\n",
      "        -4.6555e+00, -6.3739e+00,  7.4363e-01, -1.3343e+00, -7.2348e-02,\n",
      "        -2.0423e+00,  3.7000e-01, -3.5627e+00, -1.9930e+00, -1.8647e+00,\n",
      "         1.3201e+00, -3.7614e+00, -2.9709e+00, -1.0222e+00, -1.9434e+00,\n",
      "        -2.9945e+00,  2.2073e-01,  1.7212e+00, -2.3659e+00, -1.2634e+00,\n",
      "        -1.0104e+00, -4.0589e+00,  2.7511e-02,  1.4427e+00,  7.0132e-02,\n",
      "        -3.4568e+00,  1.0521e+00,  3.5129e+00,  6.4661e-01, -1.4407e+00,\n",
      "        -2.4270e-01, -3.5198e+00, -9.4175e-01, -1.0295e+00,  3.3603e-01,\n",
      "         2.0016e-01, -8.9978e-02,  2.2656e-01, -9.6323e-01, -1.7674e+00,\n",
      "         6.4672e-01, -3.3783e-02,  4.2802e+00,  5.3263e+00,  2.2384e+00,\n",
      "        -3.5597e+00,  1.1807e+00,  4.5927e-02, -2.6271e+00,  1.0758e-01,\n",
      "         1.2795e+00,  1.5737e+00,  1.1931e+00, -2.4457e+00, -8.4555e-01,\n",
      "         2.8623e+00,  2.8806e-01, -5.9179e-01, -9.3240e-01,  1.4194e+00,\n",
      "        -1.0869e+00,  8.0467e-01,  2.6026e+00, -1.2723e+00, -7.7225e-01,\n",
      "         2.2809e+00, -1.4838e+00,  5.8849e-01, -9.0864e-01, -2.9350e+00,\n",
      "        -1.5003e+00,  2.7366e+00,  2.9118e+00, -2.5303e-01, -1.1096e+00,\n",
      "         2.3799e+00,  7.6072e-01,  5.4272e+00,  4.6897e+00, -1.3155e-01,\n",
      "        -8.0122e-01, -3.1159e+00, -4.0464e+00,  6.5657e-01,  2.0462e+00,\n",
      "         2.7152e+00,  2.0442e+00,  1.9868e+00, -1.4363e+00, -2.0505e+00,\n",
      "        -1.6228e+00, -7.0752e-01,  6.1674e-01,  2.5873e+00,  2.1843e+00,\n",
      "        -6.1809e-01, -1.9566e+00, -8.5768e-01,  7.1875e-01,  2.1852e+00,\n",
      "         2.9276e-01, -2.9160e+00,  4.7730e-01,  2.0784e+00,  4.0771e+00,\n",
      "        -2.4196e-01,  1.3048e+00, -5.0637e-01, -3.4089e-01,  6.6665e-01,\n",
      "        -1.1453e+00,  1.2197e+00, -1.6476e+00,  1.0244e+00, -1.2753e-02,\n",
      "        -3.8730e+00,  1.3927e+00,  3.1080e+00,  2.1766e+00,  1.2401e-02,\n",
      "         8.3346e-01,  6.3474e-01,  6.4802e-01,  1.8221e+00, -2.7608e+00,\n",
      "        -3.6870e+00,  1.3747e+00,  9.1804e-02,  8.2602e-01, -1.5124e+00,\n",
      "         2.2673e+00,  1.3143e-01, -2.9091e+00, -1.5961e-01,  2.5890e+00,\n",
      "        -1.2447e+00, -1.2865e+00,  5.8338e+00,  1.6895e+00, -2.0549e-01,\n",
      "        -1.0211e+00, -2.4552e+00, -5.0935e-01, -5.2974e-01, -4.5986e-01,\n",
      "         2.6499e-01, -2.9162e+00, -3.0222e+00,  2.2564e-01,  1.5582e+00,\n",
      "        -1.2585e-01, -1.8132e+00,  2.3501e+00, -3.3209e+00,  4.9566e+00,\n",
      "        -4.9510e+00, -7.4566e-01,  2.1571e+00,  7.6538e-01,  2.0662e+00,\n",
      "         1.6266e+00, -1.2199e-01, -4.3637e+00, -2.1941e+00, -3.0787e-01,\n",
      "        -3.8238e+00, -1.6580e+00,  4.3411e+00,  2.7014e-01, -3.5598e-01,\n",
      "        -2.5364e-01, -4.9810e-01,  2.2959e+00,  2.2455e+00,  8.4955e-01,\n",
      "        -9.5632e-01, -1.0927e-02,  2.0118e-01,  6.2614e-01, -2.0728e+00,\n",
      "        -2.8902e+00,  1.5548e+00,  7.2549e-01, -1.0286e+00,  4.4616e-02,\n",
      "        -1.0328e+00, -6.1001e-02,  8.5735e-02,  2.3276e+00,  2.5550e+00,\n",
      "         1.4773e+00, -2.1379e+00,  8.2632e-01,  9.0537e-01,  5.8718e-01,\n",
      "         1.4047e-01,  1.2880e+00, -1.2447e+00, -3.0260e+00,  1.3479e+00,\n",
      "         6.5379e-01,  1.0272e+00,  1.3767e+00,  1.0319e+00,  4.7779e-01,\n",
      "         5.2349e-01, -8.8269e-01, -1.4962e+00, -1.1824e+00,  1.1074e+00,\n",
      "        -2.2532e+00,  2.1382e+00,  9.7307e-01, -1.0083e+00, -1.6575e+00,\n",
      "        -9.6078e-04,  1.6627e+00, -2.1514e+00, -2.3709e+00, -3.4100e+00,\n",
      "        -5.2928e-01,  1.9259e-01, -8.0650e-01,  1.6568e+00, -5.1727e-01,\n",
      "        -1.5837e-01, -8.9716e-01, -4.5829e+00,  1.4675e+00, -4.5201e-01,\n",
      "         1.2436e+00,  6.0141e-01, -3.8649e-01, -1.7420e+00, -1.5792e+00,\n",
      "         9.3821e-01,  4.2496e+00,  1.1979e+00, -2.4132e-01, -3.2378e+00,\n",
      "        -1.0954e+00, -9.6519e-02,  1.8683e+00,  1.1284e+00,  2.0860e+00,\n",
      "         9.5167e-01,  1.8403e+00,  2.5347e-01,  1.8567e-01, -1.2812e+00,\n",
      "        -6.6935e-01,  2.0522e+00,  1.4342e+00, -2.2879e-01, -8.0857e-01,\n",
      "         1.9737e+00,  1.1951e+00, -8.6991e-01, -6.7025e-01,  5.4058e-01,\n",
      "        -4.6110e-01,  2.6557e+00, -1.3025e+00, -1.6542e+00, -1.3156e+00,\n",
      "         3.2369e-01, -5.4370e-01,  1.6666e+00,  4.5700e+00, -2.3573e+00,\n",
      "         1.2469e+00,  1.1997e+00,  4.0686e+00, -7.7571e-03, -1.1327e+00,\n",
      "         1.3139e+00, -3.5417e-01, -6.1601e-01, -3.9956e+00,  1.6335e+00,\n",
      "        -1.0202e+00, -4.5716e-01,  5.7760e-01, -3.4985e+00, -1.1559e-01,\n",
      "        -4.9487e-01,  6.6963e-01, -6.2700e-01,  1.0441e+00,  3.0509e+00,\n",
      "        -3.1207e-01,  2.7649e+00,  6.2701e-01, -1.7978e+00, -2.0636e+00,\n",
      "         6.7860e-01,  1.0197e-01, -1.0705e+00,  3.0527e-01, -1.7576e+00,\n",
      "         1.1912e+00, -2.5650e+00, -2.0395e+00, -2.3385e-02, -6.6742e-01,\n",
      "        -1.3196e+00, -1.5473e+00,  1.4445e+00,  4.8465e-01, -2.8597e-01,\n",
      "         2.0573e+00,  1.8837e+00, -4.4886e-01, -5.9844e-01,  1.4961e+00,\n",
      "         4.2709e+00,  4.7192e-02, -3.9729e+00,  1.6866e+00,  1.1838e-01,\n",
      "        -2.1901e+00, -1.4564e+00,  6.3495e-01, -6.9228e-02, -1.1540e-01,\n",
      "        -2.6747e+00, -1.8754e+00, -3.3242e-02, -1.3887e+00, -5.8239e-01,\n",
      "        -2.6332e+00,  2.9077e+00,  7.7976e-01,  1.2104e+00,  1.8161e+00,\n",
      "        -1.2151e+00,  1.0008e+00,  2.1955e+00,  3.8305e+00, -1.8397e+00,\n",
      "         5.7437e-01, -3.9989e+00, -2.8352e-01,  2.3984e+00, -1.1624e+00,\n",
      "         7.7795e-01,  4.9292e+00, -1.8727e+00,  4.6069e+00, -4.2315e+00,\n",
      "         2.9490e-01, -2.9247e+00, -4.1031e-01, -2.6956e-01, -3.2782e+00,\n",
      "         5.1223e-02, -1.1451e+00,  1.9002e+00, -3.3774e+00,  3.5610e+00,\n",
      "        -5.8406e-01, -2.5785e-01,  2.5958e+00,  4.8148e-01,  3.4009e+00,\n",
      "         1.9466e+00,  4.3969e+00,  3.1025e+00,  1.8707e+00,  2.3226e+00,\n",
      "        -1.7758e+00,  4.5991e+00, -3.1057e-01,  2.5523e+00, -4.3005e-01,\n",
      "         1.1663e+00,  3.7423e+00, -3.3202e+00,  2.9863e+00,  2.3991e+00,\n",
      "         1.8163e+00,  6.2834e-01, -3.0631e+00,  1.3832e+00,  2.3446e-01,\n",
      "         4.5702e-01, -5.2095e-01, -6.3540e-01, -2.3105e+00, -9.3958e-02,\n",
      "         4.7082e-01, -4.4529e-01,  3.7371e-01, -1.8174e+00, -1.6152e+00,\n",
      "        -3.7139e-01, -5.7884e-01,  1.7545e+00,  7.7587e-01, -7.8316e-02,\n",
      "         1.6371e-02, -2.9957e+00,  1.5097e+00, -1.9589e+00, -2.1457e+00,\n",
      "         8.6147e-01,  1.3731e+00,  4.8385e+00,  1.1895e+00, -5.1667e-02,\n",
      "        -6.9186e-01, -2.4000e+00,  2.2452e+00, -2.8816e+00,  2.9604e-01,\n",
      "        -2.4871e-02, -1.6365e+00,  5.4108e-01,  7.9194e-01, -4.7306e-01,\n",
      "         3.3787e+00,  3.1170e-01, -4.7706e-01,  4.7993e-01, -2.1351e+00,\n",
      "        -1.1842e+00,  2.8761e+00, -2.5128e+00,  6.7475e-01,  4.2305e-01,\n",
      "        -1.5112e-01, -2.4046e-01,  2.6086e+00,  2.2590e+00, -1.3191e+00,\n",
      "        -1.4585e+00, -1.7858e-01,  2.4938e-02,  3.6163e+00,  1.1935e+00,\n",
      "         1.5226e+00,  1.1416e+00,  1.3794e+00,  1.2284e+00, -3.1586e+00,\n",
      "         1.8730e+00,  5.6497e-01,  3.5441e-01,  3.7591e+00, -7.5652e-01,\n",
      "         1.4521e+00,  9.2728e-01,  1.7928e+00,  1.3086e+00,  3.3280e-01,\n",
      "         2.5021e+00,  1.2359e-01, -1.9837e+00, -1.9212e+00,  2.2389e+00,\n",
      "         1.7770e+00, -1.0251e+00,  1.1154e+00, -5.6650e-01,  8.5459e-01,\n",
      "        -9.2451e-01,  1.7175e+00,  5.5464e+00, -1.1396e+00, -1.0508e+00,\n",
      "         9.4548e-01, -1.1248e+00, -4.1178e+00, -1.5001e+00, -2.4440e+00,\n",
      "        -2.2460e+00,  2.9526e-01,  6.0923e-02, -6.5617e-02, -3.6812e-01,\n",
      "        -3.9457e+00, -2.5774e-01,  1.0757e+00,  2.7218e-01,  1.2959e-01,\n",
      "         2.9834e+00,  1.5601e+00, -1.3121e+00, -3.4189e+00, -4.8114e+00,\n",
      "        -7.6685e-02,  2.9641e+00, -1.4930e+00, -1.4022e+00,  3.6878e+00,\n",
      "         2.2555e+00,  2.3702e-01,  3.8400e+00, -5.7366e-01, -2.9713e+00,\n",
      "        -1.8238e+00, -2.4695e-01, -5.4835e+00, -1.7653e+00, -1.3388e-01,\n",
      "        -3.1702e+00, -1.5837e+00, -6.3624e-01, -4.9745e-01, -1.8403e+00,\n",
      "         1.0520e+00,  1.6680e+00,  1.8475e+00,  4.0332e+00,  1.3918e+00,\n",
      "        -3.2777e+00, -4.6016e-01,  1.0744e+00, -3.8841e-01,  2.3637e+00,\n",
      "        -5.5232e-02, -1.5774e-01,  1.7894e+00,  8.5379e-01,  6.7841e-01,\n",
      "         7.2238e-01,  7.0538e-01,  3.4570e+00,  3.3763e-01, -1.2732e+00,\n",
      "        -1.6014e+00, -1.1200e-01, -5.0247e+00, -9.6371e-01, -2.9871e-01,\n",
      "        -5.2168e-01, -2.2425e+00, -3.0319e+00, -1.8120e-01, -1.8274e+00,\n",
      "        -2.9184e+00, -5.3206e+00, -4.1686e-01, -1.1471e+00,  4.0239e+00,\n",
      "         1.6161e+00, -7.2602e-01,  2.2094e+00, -3.8633e+00, -2.6026e-01,\n",
      "         1.3907e+00,  2.0413e+00,  1.4310e+00, -5.5395e-01,  8.4762e-01,\n",
      "         1.2556e+00, -2.3636e+00,  5.4899e-01,  2.3556e-01, -2.1998e+00,\n",
      "        -3.3631e+00, -2.3922e+00, -1.7042e+00,  2.6087e+00,  1.6107e+00,\n",
      "        -3.4292e-01, -4.9724e-01, -3.1873e+00, -4.1049e-01, -1.3927e+00,\n",
      "        -3.3978e-01, -1.3094e+00,  3.4263e-01,  2.9870e+00, -1.0335e+00,\n",
      "        -5.0580e-01,  7.1300e-01, -1.3783e+00, -2.2482e+00, -3.8999e+00,\n",
      "        -2.0363e+00,  1.1213e+00, -4.8775e+00,  3.2950e-01,  1.5133e+00,\n",
      "         1.1563e+00,  2.0571e+00,  5.9265e-01, -3.9987e+00, -7.2273e-01,\n",
      "         2.9365e+00, -1.7806e+00,  1.8060e+00, -2.9915e-01, -1.3354e+00,\n",
      "         4.0329e-01,  1.8246e-02, -1.7063e-01, -2.5046e+00, -2.3489e-01,\n",
      "        -5.1013e-01, -7.8850e-01,  1.9875e+00, -2.5939e-01, -2.7370e+00,\n",
      "         6.9351e-01, -1.7628e+00, -5.1118e-01, -6.8332e+00, -4.1562e+00,\n",
      "        -2.5831e+00, -1.4274e+00, -3.0766e+00,  1.6652e+00,  3.3317e+00])\n",
      "tensor([3.9432e-06, 8.5398e-08, 5.1760e-08, 2.4907e-08, 4.4774e-08, 1.2818e-06,\n",
      "        9.7240e-08, 1.9409e-05, 2.8588e-04, 1.3284e-07, 5.6362e-10, 1.6938e-08,\n",
      "        1.6867e-10, 5.6220e-09, 1.7157e-09, 4.5706e-09, 6.8330e-08, 3.3831e-07,\n",
      "        1.3440e-07, 4.5330e-09, 1.7967e-08, 3.7138e-08, 4.2395e-08, 1.3168e-07,\n",
      "        1.8854e-08, 1.1656e-07, 1.4577e-07, 7.3131e-07, 9.6811e-08, 2.3665e-06,\n",
      "        6.3864e-07, 2.5867e-07, 3.5951e-07, 1.0593e-08, 1.0324e-07, 2.6695e-08,\n",
      "        2.7501e-07, 4.4179e-08, 3.4552e-07, 1.7424e-06, 1.3845e-07, 3.4300e-08,\n",
      "        2.1779e-08, 5.1286e-08, 3.1052e-07, 1.3701e-07, 1.1088e-06, 6.2706e-08,\n",
      "        2.5006e-07, 4.4388e-07, 7.8978e-07, 8.6757e-08, 2.2199e-07, 1.6037e-07,\n",
      "        2.8245e-07, 2.5964e-08, 7.0061e-08, 3.1653e-08, 2.6324e-07, 9.0184e-08,\n",
      "        1.8563e-06, 7.2130e-09, 1.1052e-07, 5.0733e-09, 1.8347e-08, 8.7891e-09,\n",
      "        5.6629e-07, 6.6023e-08, 2.2965e-07, 7.0926e-09, 9.9400e-09, 1.8749e-07,\n",
      "        5.7097e-08, 1.3817e-08, 4.1344e-08, 1.9071e-08, 2.0919e-08, 3.3230e-08,\n",
      "        3.4339e-07, 1.7796e-06, 7.9357e-08, 2.4133e-07, 8.9070e-07, 1.4011e-06,\n",
      "        9.7474e-07, 5.6951e-08, 2.1037e-07, 1.3576e-07, 2.5472e-08, 6.6207e-06,\n",
      "        2.2114e-08, 9.5690e-09, 2.9609e-09, 3.2859e-08, 6.8756e-09, 1.5411e-09,\n",
      "        1.1226e-07, 1.7188e-08, 2.9081e-09, 5.5229e-07, 9.2050e-07, 1.0636e-08,\n",
      "        2.6386e-07, 1.0575e-08, 2.0220e-03, 3.9281e-07, 2.3327e-06, 1.8639e-08,\n",
      "        1.3075e-07, 6.2214e-09, 7.2416e-08, 5.6197e-09, 3.5750e-08, 2.9889e-07,\n",
      "        9.3501e-07, 1.5889e-07, 6.6013e-08, 2.4435e-08, 5.1575e-07, 4.6447e-07,\n",
      "        3.3275e-09, 3.3895e-07, 5.1870e-07, 4.9684e-07, 1.3974e-06, 1.1383e-08,\n",
      "        4.9683e-08, 1.5698e-07, 1.2508e-08, 1.7556e-07, 1.0642e-08, 3.5180e-08,\n",
      "        1.0886e-06, 3.8116e-09, 4.0408e-07, 1.0980e-08, 2.9603e-08, 8.4727e-09,\n",
      "        3.0664e-08, 2.3852e-10, 8.6298e-10, 1.1379e-08, 4.0187e-09, 2.5185e-08,\n",
      "        1.5032e-07, 1.5413e-07, 7.3236e-07, 5.5320e-08, 4.9083e-08, 9.0621e-08,\n",
      "        4.4039e-08, 2.2618e-04, 2.7128e-04, 2.9585e-05, 1.5561e-04, 1.9554e-06,\n",
      "        6.7419e-07, 1.4839e-03, 2.3392e-06, 4.7844e-07, 5.0102e-07, 2.2954e-07,\n",
      "        7.9801e-07, 1.0301e-07, 4.9273e-08, 1.2709e-08, 2.5387e-07, 5.1061e-08,\n",
      "        5.6924e-07, 4.8191e-05, 1.4584e-05, 5.1977e-07, 1.0219e-07, 7.2828e-05,\n",
      "        1.6818e-04, 2.2782e-06, 6.2654e-08, 5.4267e-06, 4.0539e-08, 1.7667e-06,\n",
      "        3.3434e-06, 2.9834e-08, 1.7682e-06, 6.4480e-07, 6.0646e-06, 3.7864e-05,\n",
      "        1.3578e-04, 2.6200e-07, 1.3857e-05, 3.6825e-07, 5.0494e-06, 1.2702e-07,\n",
      "        1.1793e-04, 4.4070e-05, 6.3333e-07, 1.5262e-06, 4.2686e-07, 1.4787e-06,\n",
      "        5.1579e-07, 3.2941e-04, 5.7060e-06, 7.8861e-07, 3.4724e-07, 4.8504e-03,\n",
      "        2.4977e-06, 1.8096e-06, 5.9082e-07, 4.6619e-05, 4.3138e-06, 6.0363e-07,\n",
      "        4.3455e-08, 8.8238e-07, 9.8034e-06, 2.8080e-07, 1.3212e-07, 8.0218e-06,\n",
      "        3.4595e-06, 3.7991e-06, 1.0454e-06, 2.6164e-06, 3.3942e-06, 2.1944e-07,\n",
      "        6.5653e-04, 3.9582e-03, 1.4537e-03, 5.1072e-06, 2.0670e-05, 1.5028e-04,\n",
      "        2.1811e-05, 3.3795e-04, 8.0800e-03, 1.0798e-02, 6.6662e-03, 5.2840e-06,\n",
      "        2.9246e-07, 8.0210e-05, 8.6106e-07, 2.2683e-06, 1.7506e-06, 1.7107e-05,\n",
      "        6.1678e-06, 2.1450e-06, 2.1465e-07, 3.1199e-08, 1.1563e-06, 2.6964e-07,\n",
      "        3.2196e-07, 1.2697e-05, 7.7165e-03, 3.9110e-03, 5.0790e-03, 1.9735e-06,\n",
      "        5.4126e-07, 6.9190e-06, 1.1672e-07, 1.4783e-06, 9.2845e-05, 9.8867e-03,\n",
      "        8.3030e-01, 6.9887e-02, 1.9524e-04, 1.2964e-02, 3.3976e-07, 1.9301e-03,\n",
      "        1.9642e-04, 9.5868e-07, 7.3612e-07, 3.8653e-06, 2.7651e-07, 5.8987e-05,\n",
      "        3.9501e-03, 2.6796e-06, 4.8325e-06, 4.8105e-05, 1.8579e-05, 1.9663e-07,\n",
      "        1.1442e-06, 4.2057e-05, 8.4457e-06, 4.9811e-03, 1.4540e-05, 2.6267e-05,\n",
      "        2.0979e-05, 5.4742e-04, 2.4036e-05, 3.6672e-05, 3.5777e-07, 2.4065e-05,\n",
      "        5.6698e-07, 1.9878e-06, 6.2310e-08, 1.4355e-07, 2.0988e-06, 1.1262e-07,\n",
      "        1.0031e-07, 1.2849e-07, 5.3104e-07, 3.5308e-08, 4.1689e-07, 2.2699e-07,\n",
      "        2.2900e-09, 1.2952e-08, 4.7359e-08, 8.9996e-08, 1.1326e-08, 9.8554e-09,\n",
      "        2.1910e-07, 1.0627e-07, 1.5832e-09, 2.7458e-08, 2.3348e-07, 3.2088e-07,\n",
      "        2.3553e-07, 5.0270e-07, 1.7378e-07, 1.7823e-08, 1.3587e-08, 6.6044e-08,\n",
      "        9.0809e-08, 1.3298e-09, 2.2759e-09, 9.0891e-09, 1.2525e-08, 7.6370e-09,\n",
      "        2.6513e-08, 1.7030e-07, 3.2988e-08, 7.1918e-08, 4.6442e-08, 2.5944e-07,\n",
      "        1.6392e-05, 2.5646e-04, 2.4833e-03, 7.6799e-05, 3.0090e-06, 5.1825e-05,\n",
      "        1.1265e-06, 4.7462e-07, 1.4060e-05, 4.3670e-07, 4.1302e-08, 5.4400e-06,\n",
      "        2.8962e-07, 1.3935e-08, 1.3944e-08, 5.9296e-07, 1.2218e-07, 3.5634e-08,\n",
      "        5.8646e-06, 4.4152e-08, 1.4776e-07, 3.2789e-09, 1.7723e-08, 3.1799e-07,\n",
      "        1.1685e-08, 1.0588e-04, 1.5348e-05, 1.3640e-06, 2.0940e-05, 3.0651e-05,\n",
      "        2.4481e-07, 9.4564e-06, 3.7124e-07, 1.0868e-06, 8.8360e-08, 6.2655e-08,\n",
      "        2.5217e-08, 1.8470e-08, 1.9936e-07, 1.7811e-08, 3.4516e-07, 1.7000e-06,\n",
      "        1.9770e-07, 2.0719e-06, 1.2514e-06, 6.2263e-08, 5.9024e-09, 4.7301e-06,\n",
      "        2.4375e-07, 1.9945e-07, 2.2064e-06, 3.2971e-07, 3.2204e-07, 1.2728e-06,\n",
      "        1.7046e-06, 4.6026e-09, 8.2547e-10, 1.0181e-06, 1.2746e-07, 4.5022e-07,\n",
      "        6.2792e-08, 7.0070e-07, 1.3727e-08, 6.5961e-08, 7.4989e-08, 1.8120e-06,\n",
      "        1.1253e-08, 2.4808e-08, 1.7414e-07, 6.9313e-08, 2.4229e-08, 6.0354e-07,\n",
      "        2.7061e-06, 4.5430e-08, 1.3682e-07, 1.7621e-07, 8.3577e-09, 4.9750e-07,\n",
      "        2.0484e-06, 5.1916e-07, 1.5261e-08, 1.3860e-06, 1.6236e-05, 9.2398e-07,\n",
      "        1.1459e-07, 3.7970e-07, 1.4329e-08, 1.8873e-07, 1.7288e-07, 6.7730e-07,\n",
      "        5.9125e-07, 4.4235e-07, 6.0707e-07, 1.8472e-07, 8.2659e-08, 9.2409e-07,\n",
      "        4.6792e-07, 3.4971e-05, 9.9546e-05, 4.5393e-06, 1.3768e-08, 1.5762e-06,\n",
      "        5.0675e-07, 3.4988e-08, 5.3897e-07, 1.7400e-06, 2.3351e-06, 1.5959e-06,\n",
      "        4.1946e-08, 2.0779e-07, 8.4711e-06, 6.4557e-07, 2.6781e-07, 1.9051e-07,\n",
      "        2.0012e-06, 1.6324e-07, 1.0822e-06, 6.5337e-06, 1.3561e-07, 2.2360e-07,\n",
      "        4.7360e-06, 1.0976e-07, 8.7181e-07, 1.9509e-07, 2.5715e-08, 1.0796e-07,\n",
      "        7.4701e-06, 8.9006e-06, 3.7580e-07, 1.5957e-07, 5.2289e-06, 1.0357e-06,\n",
      "        1.1012e-04, 5.2670e-05, 4.2434e-07, 2.1721e-07, 2.1461e-08, 8.4629e-09,\n",
      "        9.3323e-07, 3.7454e-06, 7.3124e-06, 3.7377e-06, 3.5295e-06, 1.1510e-07,\n",
      "        6.2278e-08, 9.5511e-08, 2.3855e-07, 8.9679e-07, 6.4344e-06, 4.3001e-06,\n",
      "        2.6086e-07, 6.8410e-08, 2.0529e-07, 9.9310e-07, 4.3038e-06, 6.4862e-07,\n",
      "        2.6209e-08, 7.8007e-07, 3.8681e-06, 2.8545e-05, 3.7998e-07, 1.7844e-06,\n",
      "        2.9170e-07, 3.4419e-07, 9.4268e-07, 1.5398e-07, 1.6388e-06, 9.3177e-08,\n",
      "        1.3482e-06, 4.7787e-07, 1.0066e-08, 1.9485e-06, 1.0830e-05, 4.2672e-06,\n",
      "        4.9004e-07, 1.1138e-06, 9.1308e-07, 9.2528e-07, 2.9936e-06, 3.0609e-08,\n",
      "        1.2123e-08, 1.9137e-06, 5.3054e-07, 1.1055e-06, 1.0666e-07, 4.6723e-06,\n",
      "        5.5198e-07, 2.6389e-08, 4.1260e-07, 6.4450e-06, 1.3940e-07, 1.3370e-07,\n",
      "        1.6536e-04, 2.6218e-06, 3.9410e-07, 1.7433e-07, 4.1548e-08, 2.9083e-07,\n",
      "        2.8496e-07, 3.0558e-07, 6.3085e-07, 2.6203e-08, 2.3569e-08, 6.0651e-07,\n",
      "        2.2991e-06, 4.2677e-07, 7.8954e-08, 5.0758e-06, 1.7481e-08, 6.8779e-05,\n",
      "        3.4251e-09, 2.2962e-07, 4.1848e-06, 1.0405e-06, 3.8210e-06, 2.4618e-06,\n",
      "        4.2842e-07, 6.1617e-09, 5.3947e-08, 3.5574e-07, 1.0572e-08, 9.2213e-08,\n",
      "        3.7167e-05, 6.3411e-07, 3.3904e-07, 3.7557e-07, 2.9412e-07, 4.8078e-06,\n",
      "        4.5712e-06, 1.1319e-06, 1.8600e-07, 4.7874e-07, 5.9186e-07, 9.0526e-07,\n",
      "        6.0906e-08, 2.6893e-08, 2.2913e-06, 9.9981e-07, 1.7304e-07, 5.0608e-07,\n",
      "        1.7230e-07, 4.5536e-07, 5.2733e-07, 4.9626e-06, 6.2295e-06, 2.1205e-06,\n",
      "        5.7065e-08, 1.1059e-06, 1.1969e-06, 8.7067e-07, 5.5699e-07, 1.7548e-06,\n",
      "        1.3940e-07, 2.3478e-08, 1.8630e-06, 9.3064e-07, 1.3520e-06, 1.9175e-06,\n",
      "        1.3583e-06, 7.8045e-07, 8.1695e-07, 2.0021e-07, 1.0841e-07, 1.4837e-07,\n",
      "        1.4647e-06, 5.0849e-08, 4.1064e-06, 1.2807e-06, 1.7657e-07, 9.2259e-08,\n",
      "        4.8353e-07, 2.5524e-06, 5.6300e-08, 4.5202e-08, 1.5991e-08, 2.8509e-07,\n",
      "        5.8679e-07, 2.1607e-07, 2.5374e-06, 2.8854e-07, 4.1311e-07, 1.9734e-07,\n",
      "        4.9488e-09, 2.0997e-06, 3.0799e-07, 1.6786e-06, 8.8314e-07, 3.2885e-07,\n",
      "        8.4783e-08, 9.9775e-08, 1.2368e-06, 3.3919e-05, 1.6036e-06, 3.8022e-07,\n",
      "        1.8996e-08, 1.6186e-07, 4.3947e-07, 3.1349e-06, 1.4959e-06, 3.8973e-06,\n",
      "        1.2536e-06, 3.0484e-06, 6.2363e-07, 5.8275e-07, 1.3441e-07, 2.4783e-07,\n",
      "        3.7679e-06, 2.0309e-06, 3.8502e-07, 2.1562e-07, 3.4836e-06, 1.5991e-06,\n",
      "        2.0279e-07, 2.4760e-07, 8.3103e-07, 3.0521e-07, 6.8899e-06, 1.3158e-07,\n",
      "        9.2566e-08, 1.2987e-07, 6.6899e-07, 2.8101e-07, 2.5624e-06, 4.6730e-05,\n",
      "        4.5822e-08, 1.6842e-06, 1.6064e-06, 2.8301e-05, 4.8026e-07, 1.5593e-07,\n",
      "        1.8008e-06, 3.3965e-07, 2.6141e-07, 8.9039e-09, 2.4791e-06, 1.7449e-07,\n",
      "        3.0641e-07, 8.6237e-07, 1.4638e-08, 4.3117e-07, 2.9507e-07, 9.4550e-07,\n",
      "        2.5855e-07, 1.3750e-06, 1.0229e-05, 3.5425e-07, 7.6846e-06, 9.0605e-07,\n",
      "        8.0181e-08, 6.1466e-08, 9.5402e-07, 5.3596e-07, 1.6593e-07, 6.5678e-07,\n",
      "        8.3473e-08, 1.5929e-06, 3.7230e-08, 6.2966e-08, 4.7281e-07, 2.4831e-07,\n",
      "        1.2934e-07, 1.0300e-07, 2.0521e-06, 7.8583e-07, 3.6362e-07, 3.7873e-06,\n",
      "        3.1835e-06, 3.0896e-07, 2.6604e-07, 2.1607e-06, 3.4649e-05, 5.0739e-07,\n",
      "        9.1084e-09, 2.6142e-06, 5.4482e-07, 5.4164e-08, 1.1281e-07, 9.1327e-07,\n",
      "        4.5163e-07, 4.3125e-07, 3.3360e-08, 7.4191e-08, 4.6817e-07, 1.2071e-07,\n",
      "        2.7034e-07, 3.4775e-08, 8.8644e-06, 1.0556e-06, 1.6237e-06, 2.9757e-06,\n",
      "        1.4359e-07, 1.3167e-06, 4.3485e-06, 2.2305e-05, 7.6890e-08, 8.5959e-07,\n",
      "        8.8746e-09, 3.6451e-07, 5.3267e-06, 1.5136e-07, 1.0537e-06, 6.6922e-05,\n",
      "        7.4395e-08, 4.8483e-05, 7.0329e-09, 6.5001e-07, 2.5980e-08, 3.2111e-07,\n",
      "        3.6964e-07, 1.8244e-08, 5.0944e-07, 1.5400e-07, 3.2367e-06, 1.6521e-08,\n",
      "        1.7035e-05, 2.6989e-07, 3.7399e-07, 6.4892e-06, 7.8334e-07, 1.4516e-05,\n",
      "        3.3903e-06, 3.9301e-05, 1.0771e-05, 3.1424e-06, 4.9378e-06, 8.1967e-08,\n",
      "        4.8105e-05, 3.5478e-07, 6.2126e-06, 3.1483e-07, 1.5537e-06, 2.0423e-05,\n",
      "        1.7494e-08, 9.5894e-06, 5.3306e-06, 2.9761e-06, 9.0725e-07, 2.2622e-08,\n",
      "        1.9300e-06, 6.1188e-07, 7.6441e-07, 2.8747e-07, 2.5639e-07, 4.8020e-08,\n",
      "        4.4059e-07, 7.7503e-07, 3.1007e-07, 7.0330e-07, 7.8625e-08, 9.6246e-08,\n",
      "        3.3385e-07, 2.7130e-07, 2.7978e-06, 1.0515e-06, 4.4754e-07, 4.9199e-07,\n",
      "        2.4201e-08, 2.1902e-06, 6.8252e-08, 5.6619e-08, 1.1455e-06, 1.9107e-06,\n",
      "        6.1120e-05, 1.5901e-06, 4.5963e-07, 2.4231e-07, 4.3907e-08, 4.5700e-06,\n",
      "        2.7126e-08, 6.5075e-07, 4.7211e-07, 9.4213e-08, 8.3144e-07, 1.0685e-06,\n",
      "        3.0158e-07, 1.4197e-05, 6.6102e-07, 3.0037e-07, 7.8212e-07, 5.7222e-08,\n",
      "        1.4809e-07, 8.5883e-06, 3.9222e-08, 9.5035e-07, 7.3888e-07, 4.1611e-07,\n",
      "        3.8055e-07, 6.5726e-06, 4.6336e-06, 1.2941e-07, 1.1257e-07, 4.0485e-07,\n",
      "        4.9622e-07, 1.8005e-05, 1.5965e-06, 2.2188e-06, 1.5157e-06, 1.9226e-06,\n",
      "        1.6532e-06, 2.0562e-08, 3.1499e-06, 8.5155e-07, 6.8986e-07, 2.0769e-05,\n",
      "        2.2714e-07, 2.0677e-06, 1.2234e-06, 2.9070e-06, 1.7912e-06, 6.7512e-07,\n",
      "        5.9087e-06, 5.4767e-07, 6.6580e-08, 7.0871e-08, 4.5414e-06, 2.8613e-06,\n",
      "        1.7365e-07, 1.4766e-06, 2.7467e-07, 1.1376e-06, 1.9202e-07, 2.6962e-06,\n",
      "        1.2406e-04, 1.5485e-07, 1.6923e-07, 1.2458e-06, 1.5717e-07, 7.8793e-09,\n",
      "        1.0798e-07, 4.2017e-08, 5.1216e-08, 6.5024e-07, 5.1440e-07, 4.5326e-07,\n",
      "        3.3495e-07, 9.3594e-09, 3.7403e-07, 1.4191e-06, 6.3540e-07, 5.5097e-07,\n",
      "        9.5613e-06, 2.3034e-06, 1.3032e-07, 1.5851e-08, 3.9380e-09, 4.4827e-07,\n",
      "        9.3790e-06, 1.0875e-07, 1.1909e-07, 1.9340e-05, 4.6175e-06, 6.1345e-07,\n",
      "        2.2519e-05, 2.7271e-07, 2.4799e-08, 7.8122e-08, 3.7809e-07, 2.0109e-09,\n",
      "        8.2828e-08, 4.2335e-07, 2.0325e-08, 9.9324e-08, 2.5617e-07, 2.9431e-07,\n",
      "        7.6842e-08, 1.3858e-06, 2.5660e-06, 3.0705e-06, 2.7318e-05, 1.9467e-06,\n",
      "        1.8253e-08, 3.0549e-07, 1.4173e-06, 3.2822e-07, 5.1450e-06, 4.5799e-07,\n",
      "        4.1337e-07, 2.8971e-06, 1.1367e-06, 9.5384e-07, 9.9671e-07, 9.7991e-07,\n",
      "        1.5353e-05, 6.7838e-07, 1.3548e-07, 9.7582e-08, 4.3272e-07, 3.1815e-09,\n",
      "        1.8463e-07, 3.5902e-07, 2.8726e-07, 5.1395e-08, 2.3341e-08, 4.0379e-07,\n",
      "        7.7843e-08, 2.6144e-08, 2.3666e-09, 3.1901e-07, 1.5370e-07, 2.7065e-05,\n",
      "        2.4361e-06, 2.3417e-07, 4.4093e-06, 1.0163e-08, 3.7309e-07, 1.9445e-06,\n",
      "        3.7269e-06, 2.0246e-06, 2.7814e-07, 1.1297e-06, 1.6989e-06, 4.5535e-08,\n",
      "        8.3805e-07, 6.1256e-07, 5.3640e-08, 1.6760e-08, 4.4251e-08, 8.8049e-08,\n",
      "        6.5732e-06, 2.4231e-06, 3.4349e-07, 2.9437e-07, 1.9981e-08, 3.2105e-07,\n",
      "        1.2023e-07, 3.4457e-07, 1.3067e-07, 6.8178e-07, 9.5957e-06, 1.7218e-07,\n",
      "        2.9186e-07, 9.8740e-07, 1.2197e-07, 5.1106e-08, 9.7978e-09, 6.3165e-08,\n",
      "        1.4853e-06, 3.6861e-09, 6.7289e-07, 2.1981e-06, 1.5383e-06, 3.7863e-06,\n",
      "        8.7544e-07, 8.8763e-09, 2.3495e-07, 9.1230e-06, 8.1575e-08, 2.9458e-06,\n",
      "        3.5886e-07, 1.2731e-07, 7.2442e-07, 4.9291e-07, 4.0807e-07, 3.9548e-08,\n",
      "        3.8268e-07, 2.9060e-07, 2.1999e-07, 3.5320e-06, 3.7342e-07, 3.1346e-08,\n",
      "        9.6834e-07, 8.3035e-08, 2.9030e-07, 5.2149e-10, 7.5826e-09, 3.6560e-08,\n",
      "        1.1612e-07, 2.2320e-08, 2.5588e-06, 1.3546e-05])\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a086efa-a24f-432f-be15-c41397fbf9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-01 02:30:23--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-08-01 02:30:23 (10.8 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6816dfcc-0980-4de0-9b88-282e51a96807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.8303049206733704\n",
      "Pomeranian 0.06988745182752609\n",
      "keeshond 0.012964102439582348\n",
      "collie 0.010797764174640179\n",
      "Great Pyrenees 0.009886696003377438\n"
     ]
    }
   ],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a6cee3-a62d-4bf3-95a2-b71543dbee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2916fcf0-86a3-449c-bea6-c9eca78b5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/hub.py:266: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COCO annotations.\n",
      "Downloading finished.\n"
     ]
    }
   ],
   "source": [
    "# 結果可視化用にCOCOのアノテーションラベルリストを取得\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
    "classes_to_labels = utils.get_coco_object_dictionary()\n",
    "\n",
    "# 結果可視化用関数定義\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# The utility plots the images and predicted bounding boxes (with confidence scores).\n",
    "def plot_results(best_results):\n",
    "    for image_idx in range(len(best_results)):\n",
    "        fig, ax = plt.subplots(1)\n",
    "        # Show original, denormalized image...\n",
    "        image = inputs[image_idx] / 2 + 0.5\n",
    "        ax.imshow(image)\n",
    "        # ...with detections\n",
    "        bboxes, classes, confidences = best_results[image_idx]\n",
    "        for idx in range(len(bboxes)):\n",
    "            left, bot, right, top = bboxes[idx]\n",
    "            x, y, w, h = [val * 300 for val in [left, bot, right - left, top - bot]]\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x, y, \"{} {:.0f}%\".format(classes_to_labels[classes[idx] - 1], confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f66e62-653c-4bcd-97ab-d999793142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用する画像リストを作成（ローカルにある画像へのパスを指定しても可）\n",
    "uris = [\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000037777.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000252219.jpg',\n",
    "    'https://rt-net.jp/wp-content/uploads/2020/10/edu_seminar_image_raspimouse.png-600x400.jpg',\n",
    "    'https://rt-net.jp/mobility/wp-content/uploads/2022/03/s-IMG_2553_b.jpg',\n",
    "    'https://ultralytics.com/images/zidane.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3458baf-3837-4072-8f8d-301a84215c60",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 画像取得しネットワークへ入力できるように変換\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [utils\u001b[38;5;241m.\u001b[39mprepare_input(uri) \u001b[38;5;28;01mfor\u001b[39;00m uri \u001b[38;5;129;01min\u001b[39;00m uris]\n\u001b[0;32m----> 3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Detection/SSD/ssd/entrypoints.py:113\u001b[0m, in \u001b[0;36mnvidia_ssd_processing_utils.<locals>.Processing.prepare_tensor\u001b[0;34m(inputs, fp16)\u001b[0m\n\u001b[1;32m    111\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(NCHW)\n\u001b[1;32m    112\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 113\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp16:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# 画像取得しネットワークへ入力できるように変換\n",
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "tensor = utils.prepare_tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67184f-08eb-42c2-b6e5-4e3e01a2af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 物体検出\n",
    "with torch.no_grad():\n",
    "    detections_batch = ssd_model(tensor)\n",
    "\n",
    "results_per_input = utils.decode_results(detections_batch)\n",
    "best_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f3367-710b-4b4c-9ea2-7ec568ae2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果可視化\n",
    "plot_results(best_results_per_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae88e86-7246-425b-9462-038a0395f061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
